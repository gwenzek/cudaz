//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_35
.address_size 64

	// .globl	global_reduce_kernel    // -- Begin function global_reduce_kernel
.func  (.param .b32 func_retval0) _ZL9atomicAddPii
(
	.param .b64 _ZL9atomicAddPii_param_0,
	.param .b32 _ZL9atomicAddPii_param_1
)
;
.global .align 1 .b8 threadIdx[1];
.global .align 1 .b8 blockDim[1];
.global .align 1 .b8 blockIdx[1];
.extern .shared .align 4 .b8 sdata[];
                                        // @global_reduce_kernel
.visible .entry global_reduce_kernel(
	.param .u64 global_reduce_kernel_param_0,
	.param .u64 global_reduce_kernel_param_1
)
{
	.local .align 8 .b8 	__local_depot0[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<19>;

// %bb.0:
	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [global_reduce_kernel_param_1];
	ld.param.u64 	%rd1, [global_reduce_kernel_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.global.u64 	%rd6, %rd5;
	st.u64 	[%SP+0], %rd6;
	st.u64 	[%SP+8], %rd4;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.lo.s32 	%r4, %r2, %r3;
	add.s32 	%r5, %r1, %r4;
	st.u32 	[%SP+16], %r5;
	st.u32 	[%SP+20], %r1;
	shr.u32 	%r6, %r2, 1;
	st.u32 	[%SP+24], %r6;
	bra.uni 	LBB0_1;
LBB0_1:                                 // =>This Inner Loop Header: Depth=1
	ld.u32 	%r7, [%SP+24];
	setp.eq.s32 	%p1, %r7, 0;
	@%p1 bra 	LBB0_6;
	bra.uni 	LBB0_2;
LBB0_2:                                 //   in Loop: Header=BB0_1 Depth=1
	ld.u32 	%r10, [%SP+20];
	ld.u32 	%r11, [%SP+24];
	setp.ge.u32 	%p3, %r10, %r11;
	@%p3 bra 	LBB0_4;
	bra.uni 	LBB0_3;
LBB0_3:                                 //   in Loop: Header=BB0_1 Depth=1
	ld.u64 	%rd14, [%SP+8];
	ld.u32 	%r12, [%SP+16];
	ld.u32 	%r13, [%SP+24];
	add.s32 	%r14, %r12, %r13;
	mul.wide.u32 	%rd15, %r14, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.f32 	%f2, [%rd16];
	mul.wide.s32 	%rd17, %r12, 4;
	add.s64 	%rd18, %rd14, %rd17;
	ld.f32 	%f3, [%rd18];
	add.rn.f32 	%f4, %f3, %f2;
	st.f32 	[%rd18], %f4;
	bra.uni 	LBB0_4;
LBB0_4:                                 //   in Loop: Header=BB0_1 Depth=1
	bar.sync 	0;
	bra.uni 	LBB0_5;
LBB0_5:                                 //   in Loop: Header=BB0_1 Depth=1
	ld.u32 	%r15, [%SP+24];
	shr.u32 	%r16, %r15, 1;
	st.u32 	[%SP+24], %r16;
	bra.uni 	LBB0_1;
LBB0_6:
	ld.u32 	%r8, [%SP+20];
	setp.ne.s32 	%p2, %r8, 0;
	@%p2 bra 	LBB0_8;
	bra.uni 	LBB0_7;
LBB0_7:
	ld.u64 	%rd7, [%SP+8];
	ld.s32 	%rd8, [%SP+16];
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd7, %rd9;
	ld.f32 	%f1, [%rd10];
	ld.u64 	%rd11, [%SP+0];
	mov.u32 	%r9, %ctaid.x;
	mul.wide.u32 	%rd12, %r9, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.f32 	[%rd13], %f1;
	bra.uni 	LBB0_8;
LBB0_8:
	ret;
                                        // -- End function
}
	// .globl	shmem_reduce_kernel     // -- Begin function shmem_reduce_kernel
.visible .entry shmem_reduce_kernel(
	.param .u64 shmem_reduce_kernel_param_0,
	.param .u64 shmem_reduce_kernel_param_1
)                                       // @shmem_reduce_kernel
{
	.local .align 8 .b8 	__local_depot1[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<27>;

// %bb.0:
	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [shmem_reduce_kernel_param_1];
	ld.param.u64 	%rd1, [shmem_reduce_kernel_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.global.u64 	%rd6, %rd5;
	st.u64 	[%SP+0], %rd6;
	st.u64 	[%SP+8], %rd4;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.lo.s32 	%r4, %r2, %r3;
	add.s32 	%r5, %r1, %r4;
	st.u32 	[%SP+16], %r5;
	st.u32 	[%SP+20], %r1;
	ld.u64 	%rd7, [%SP+8];
	ld.s32 	%rd8, [%SP+16];
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd7, %rd9;
	ld.f32 	%f1, [%rd10];
	ld.s32 	%rd11, [%SP+20];
	mov.u64 	%rd12, sdata;
	cvta.shared.u64 	%rd13, %rd12;
	shl.b64 	%rd14, %rd11, 2;
	add.s64 	%rd15, %rd13, %rd14;
	st.f32 	[%rd15], %f1;
	bar.sync 	0;
	shr.u32 	%r6, %r2, 1;
	st.u32 	[%SP+24], %r6;
	bra.uni 	LBB1_1;
LBB1_1:                                 // =>This Inner Loop Header: Depth=1
	ld.u32 	%r7, [%SP+24];
	setp.eq.s32 	%p1, %r7, 0;
	@%p1 bra 	LBB1_6;
	bra.uni 	LBB1_2;
LBB1_2:                                 //   in Loop: Header=BB1_1 Depth=1
	ld.u32 	%r10, [%SP+20];
	ld.u32 	%r11, [%SP+24];
	setp.ge.u32 	%p3, %r10, %r11;
	@%p3 bra 	LBB1_4;
	bra.uni 	LBB1_3;
LBB1_3:                                 //   in Loop: Header=BB1_1 Depth=1
	ld.u32 	%r12, [%SP+20];
	ld.u32 	%r13, [%SP+24];
	add.s32 	%r14, %r12, %r13;
	mul.wide.u32 	%rd21, %r14, 4;
	mov.u64 	%rd22, sdata;
	cvta.shared.u64 	%rd23, %rd22;
	add.s64 	%rd24, %rd23, %rd21;
	ld.f32 	%f3, [%rd24];
	mul.wide.s32 	%rd25, %r12, 4;
	add.s64 	%rd26, %rd23, %rd25;
	ld.f32 	%f4, [%rd26];
	add.rn.f32 	%f5, %f4, %f3;
	st.f32 	[%rd26], %f5;
	bra.uni 	LBB1_4;
LBB1_4:                                 //   in Loop: Header=BB1_1 Depth=1
	bar.sync 	0;
	bra.uni 	LBB1_5;
LBB1_5:                                 //   in Loop: Header=BB1_1 Depth=1
	ld.u32 	%r15, [%SP+24];
	shr.u32 	%r16, %r15, 1;
	st.u32 	[%SP+24], %r16;
	bra.uni 	LBB1_1;
LBB1_6:
	ld.u32 	%r8, [%SP+20];
	setp.ne.s32 	%p2, %r8, 0;
	@%p2 bra 	LBB1_8;
	bra.uni 	LBB1_7;
LBB1_7:
	mov.u64 	%rd16, sdata;
	cvta.shared.u64 	%rd17, %rd16;
	ld.f32 	%f2, [%rd17];
	ld.u64 	%rd18, [%SP+0];
	mov.u32 	%r9, %ctaid.x;
	mul.wide.u32 	%rd19, %r9, 4;
	add.s64 	%rd20, %rd18, %rd19;
	st.f32 	[%rd20], %f2;
	bra.uni 	LBB1_8;
LBB1_8:
	ret;
                                        // -- End function
}
	// .globl	naive_histo             // -- Begin function naive_histo
.visible .entry naive_histo(
	.param .u64 naive_histo_param_0,
	.param .u64 naive_histo_param_1,
	.param .u32 naive_histo_param_2
)                                       // @naive_histo
{
	.local .align 8 .b8 	__local_depot2[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<15>;

// %bb.0:
	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [naive_histo_param_2];
	ld.param.u64 	%rd2, [naive_histo_param_1];
	ld.param.u64 	%rd1, [naive_histo_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.global.u64 	%rd6, %rd5;
	st.u64 	[%SP+0], %rd6;
	st.u64 	[%SP+8], %rd4;
	st.u32 	[%SP+16], %r1;
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mul.lo.s32 	%r5, %r3, %r4;
	add.s32 	%r6, %r2, %r5;
	st.u32 	[%SP+20], %r6;
	ld.u64 	%rd7, [%SP+8];
	ld.s32 	%rd8, [%SP+20];
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd7, %rd9;
	ld.u32 	%r7, [%rd10];
	st.u32 	[%SP+24], %r7;
	ld.u32 	%r8, [%SP+24];
	ld.u32 	%r9, [%SP+16];
	rem.s32 	%r10, %r8, %r9;
	st.u32 	[%SP+28], %r10;
	ld.u64 	%rd11, [%SP+0];
	ld.s32 	%rd12, [%SP+28];
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd11, %rd13;
	ld.u32 	%r11, [%rd14];
	add.s32 	%r12, %r11, 1;
	st.u32 	[%rd14], %r12;
	ret;
                                        // -- End function
}
	// .globl	simple_histo            // -- Begin function simple_histo
.visible .entry simple_histo(
	.param .u64 simple_histo_param_0,
	.param .u64 simple_histo_param_1,
	.param .u32 simple_histo_param_2
)                                       // @simple_histo
{
	.local .align 8 .b8 	__local_depot3[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<15>;

// %bb.0:
	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [simple_histo_param_2];
	ld.param.u64 	%rd2, [simple_histo_param_1];
	ld.param.u64 	%rd1, [simple_histo_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.global.u64 	%rd6, %rd5;
	st.u64 	[%SP+0], %rd6;
	st.u64 	[%SP+8], %rd4;
	st.u32 	[%SP+16], %r1;
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mul.lo.s32 	%r5, %r3, %r4;
	add.s32 	%r6, %r2, %r5;
	st.u32 	[%SP+20], %r6;
	ld.u64 	%rd7, [%SP+8];
	ld.s32 	%rd8, [%SP+20];
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd7, %rd9;
	ld.u32 	%r7, [%rd10];
	st.u32 	[%SP+24], %r7;
	ld.u32 	%r8, [%SP+24];
	ld.u32 	%r9, [%SP+16];
	rem.s32 	%r10, %r8, %r9;
	st.u32 	[%SP+28], %r10;
	ld.u64 	%rd11, [%SP+0];
	ld.s32 	%rd12, [%SP+28];
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd11, %rd13;
	mov.u32 	%r11, 1;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r11;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZL9atomicAddPii, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r12, [retval0+0];
	} // callseq 0
	ret;
                                        // -- End function
}
.func  (.param .b32 func_retval0) _ZL9atomicAddPii(
	.param .b64 _ZL9atomicAddPii_param_0,
	.param .b32 _ZL9atomicAddPii_param_1
)                                       // -- Begin function _ZL9atomicAddPii
                                        // @_ZL9atomicAddPii
{
	.local .align 8 .b8 	__local_depot4[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<4>;

// %bb.0:
	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [_ZL9atomicAddPii_param_1];
	ld.param.u64 	%rd1, [_ZL9atomicAddPii_param_0];
	st.u64 	[%SP+16], %rd1;
	st.u32 	[%SP+24], %r1;
	ld.u64 	%rd2, [%SP+16];
	ld.u32 	%r2, [%SP+24];
	st.u64 	[%SP+0], %rd2;
	st.u32 	[%SP+8], %r2;
	ld.u64 	%rd3, [%SP+0];
	ld.u32 	%r3, [%SP+8];
	atom.add.u32 	%r4, [%rd3], %r3;
	st.param.b32 	[func_retval0+0], %r4;
	ret;
                                        // -- End function
}
